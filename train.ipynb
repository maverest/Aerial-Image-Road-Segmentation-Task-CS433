{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Import and Directory Path**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mathi\\anaconda3\\envs\\ML_torch\\lib\\site-packages\\albumentations\\__init__.py:13: UserWarning: A new version of Albumentations is available: 1.4.23 (you have 1.4.18). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n"
     ]
    }
   ],
   "source": [
    "%cd code\n",
    "import torch\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from model import *\n",
    "import os, sys\n",
    "from torch.utils.data import DataLoader\n",
    "from my_dataset import RoadDataset\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "import cv2\n",
    "from perf_tracker import TrainingTracker\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "import train_helpers as th\n",
    "from torch.nn.functional import sigmoid\n",
    "from PIL import Image\n",
    "import matplotlib.image as mpimg\n",
    "from torch.optim.lr_scheduler import LambdaLR \n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "\n",
    "\n",
    "%run -i model.py\n",
    "%run -i my_dataset.py\n",
    "%run -i perf_tracker.py\n",
    "%run -i train_helpers.py\n",
    "\n",
    "\n",
    "TRAIN_IMG_DIR = \"Data/training/train/images\"\n",
    "TRAIN_MASK_DIR = \"Data/training/train/groundtruth\"\n",
    "VAL_IMG_DIR = \"Data/training/val/images\"\n",
    "VAL_MASK_DIR = \"Data/training/val/groundtruth\"\n",
    "ROOT_DIR = os.path.join('Data', 'training')\n",
    "IMG_DIR = os.path.join(ROOT_DIR, 'images')\n",
    "MASK_DIR= os.path.join(ROOT_DIR, 'groundtruth')\n",
    "TRAIN_IMG_DIR = VAL_IMG_DIR\n",
    "TRAIN_MASK_DIR = VAL_MASK_DIR\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Training Cst**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialize network with kaiming\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "PATCH_SIZE = 400 #608 TO MODIFY AGAIN WHEN TRAINING WITH 400X400\n",
    "STRIDE = None\n",
    "AUG_FACTOR = (0,0) # If 1 ; proba des deux flips\n",
    "\n",
    "\n",
    "BATCH_SIZE = 2 #4\n",
    "LR_MODIFIER = False #True to use warm up Lr\n",
    "NUM_EPOCH = 3\n",
    "LEARNING_RATE = 2e-3 #For Att-Unet\n",
    "#LEARNING_RATE = 1e-3 #For Jaccard\n",
    "#LEARNING_RATE = 5e-4\n",
    "CLAHE = True\n",
    "Plateau = True #True to use ReduceLROnPlateau\n",
    "TRACK_F1 = True #True to track F1 score and plot\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "model = AttU_Net(img_ch=3, output_ch=1).to(device)\n",
    "init_weights(model, init_type='kaiming', gain=0.02) #TO COMMENT WHEN OTHER MODELS AS UNET\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "CRITERION = 'BCE_loss'\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE) #, weight_decay=1e-4)\n",
    "OPTIMIZER = 'Adam'\n",
    "\n",
    "\n",
    "#KERNEL_SIZE = 25\n",
    "#criterion = BCEDicePenalizeBorderLoss(kernel_size= KERNEL_SIZE)\n",
    "#CRITERION = f'BCEDicePenalizeBorderLoss_Kernel_{KERNEL_SIZE}'\n",
    "#pos_weight = torch.tensor([4.35], dtype=torch.float32).to(device)\n",
    "#criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "#criterion = IoULoss()\n",
    "#CRITERION = 'Jaccard'\n",
    "#optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "#OPTIMIZER = 'AdamW'\n",
    "\n",
    "\n",
    "\n",
    "#CRITERION = f'WeightedBCELoss\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Loading Preprocessing of Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_transform = A.Compose([\n",
    "    A.Normalize(mean=[0.0, 0.0, 0.0], std=[1.0, 1.0, 1.0], max_pixel_value=255.0),\n",
    "    ToTensorV2(),\n",
    "    ])\n",
    "\n",
    "\n",
    "val_transform = A.Compose([\n",
    "        A.Normalize(mean=[0.0, 0.0, 0.0],std=[1.0, 1.0, 1.0],max_pixel_value=255.0,),\n",
    "        ToTensorV2()\n",
    "        ])\n",
    "\n",
    "train_dataset = RoadDataset(\n",
    "    image_dir=TRAIN_IMG_DIR,\n",
    "    mask_dir=TRAIN_MASK_DIR,\n",
    "    transform=train_transform,\n",
    "    patch_size=PATCH_SIZE,\n",
    "    stride=STRIDE,\n",
    "    num_augmentations=AUG_FACTOR,\n",
    "    CLAHE = CLAHE\n",
    ")\n",
    "\n",
    "\n",
    "val_dataset = RoadDataset(\n",
    "    image_dir=VAL_IMG_DIR,\n",
    "    mask_dir=VAL_MASK_DIR,\n",
    "    transform=val_transform,\n",
    "    patch_size=PATCH_SIZE,\n",
    "    CLAHE = CLAHE\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size= len(val_dataset), \n",
    "    shuffle=False, \n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "val_img,val_mask = th.extract_val_image_mask(val_img_dir=VAL_IMG_DIR, val_mask_dir=VAL_MASK_DIR)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Tracker**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plateau\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mathi\\anaconda3\\envs\\ML_torch\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if LR_MODIFIER:\n",
    "    max_lr = 2e-3 #2e-2 For all except jaccard\n",
    "    total_steps = NUM_EPOCH * len(train_loader)\n",
    "    pct_start = 0.3\n",
    "    anneal_strategy = 'cos'\n",
    "    div_factor = 25.0\n",
    "\n",
    "    scheduler = OneCycleLR(\n",
    "        optimizer=optimizer,\n",
    "        max_lr= max_lr,\n",
    "        total_steps=NUM_EPOCH * len(train_loader),\n",
    "        pct_start=0.3,\n",
    "        anneal_strategy='cos',\n",
    "        div_factor=25.0,\n",
    "        final_div_factor= 0.05, #0.09,\n",
    "    )\n",
    "    arg = {\"max_lr\": max_lr, \"total_steps\": total_steps, \"pct_start\": pct_start, \"anneal_strategy\": anneal_strategy, \"div_factor\": div_factor}\n",
    "    tracker = TrainingTracker(hyperparams={\"NUM_EPOCH\": NUM_EPOCH, \"learning_rate\": LEARNING_RATE, \"batch_size\": BATCH_SIZE, \"optimizer\": OPTIMIZER, \"criterion\": CRITERION, \"LR_MODIFIER\": arg, \"patch_size\" : PATCH_SIZE })\n",
    "else :\n",
    "  if Plateau:\n",
    "    print(\"Plateau\")\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.9, patience=3, verbose=True, min_lr=1e-4) #patience 2??\n",
    "    tracker = TrainingTracker(hyperparams={\"NUM_EPOCH\": NUM_EPOCH, \"learning_rate\": LEARNING_RATE, \"batch_size\": BATCH_SIZE, \"optimizer\":OPTIMIZER, \"criterion\": CRITERION,\"LR_MODIFIER\" : \"redce_on_plateau\",\"patch_size\" : PATCH_SIZE})\n",
    "  else :\n",
    "\n",
    "    scheduler = StepLR(optimizer, step_size= np.ceil((len(train_dataset)//BATCH_SIZE * NUM_EPOCH * .15 )), gamma=0.7) #np.ceil((len(train_dataset)//BATCH_SIZE * NUM_EPOCH * .15))\n",
    "\n",
    "    tracker = TrainingTracker(hyperparams={\"NUM_EPOCH\": NUM_EPOCH, \"learning_rate\": LEARNING_RATE, \"batch_size\": BATCH_SIZE, \"optimizer\":OPTIMIZER, \"criterion\": CRITERION,\"LR_MODIFIER\" : \"lr_step\",\"patch_size\" : PATCH_SIZE})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Training and Validation Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, loader, criterion, optimizer, device, scheduler=None, Plateau = False):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    lr_sum = 0.0\n",
    "    loop = tqdm(loader, desc=\"Training\", leave=False)\n",
    "    num_batches = len(loader)\n",
    "    for images, masks in loop:\n",
    "        images, masks = images.to(device), masks.to(device)\n",
    "        masks = masks.unsqueeze(1)\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, masks)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if scheduler is not None and not Plateau :\n",
    "            scheduler.step()\n",
    "            current_lr = scheduler.get_last_lr()[0]\n",
    "            lr_sum += current_lr\n",
    "            loop.set_postfix(loss=loss.item(), lr=current_lr)\n",
    "        else:\n",
    "            loop.set_postfix(loss=loss.item())\n",
    "\n",
    "        train_loss += loss.item()\n",
    "    avg_loss = train_loss / num_batches\n",
    "    avg_lr = lr_sum / num_batches if (scheduler is not None) and (not Plateau) else None\n",
    "\n",
    "    return avg_loss, avg_lr\n",
    "\n",
    "def validate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    loop = tqdm(loader, desc=\"Validating\", leave=False)\n",
    "    all_preds = []\n",
    "    with torch.no_grad():\n",
    "        for images, masks in loop:\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "            masks = masks.unsqueeze(1)\n",
    "            outputs = model(images)\n",
    "            all_preds.append(outputs.cpu().squeeze(1))\n",
    "            loss = criterion(outputs, masks) #Sigmoid is already included in the loss\n",
    "            val_loss += loss.item()\n",
    "            loop.set_postfix(loss=loss.item())\n",
    "    all_preds = torch.cat(all_preds, dim=0)\n",
    "    return val_loss / len(loader), all_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **MODEL TRAINING**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if TRACK_F1 :\n",
    "  max_f1 = 0\n",
    "  f1_list = []\n",
    "past_val_loss = 1000\n",
    "\n",
    "\n",
    "for epoch in range(NUM_EPOCH):\n",
    "    print(f\"Epoch {epoch+1}/{NUM_EPOCH}\")\n",
    "\n",
    "    # Training Phase\n",
    "    if scheduler is not None :\n",
    "      train_loss, avg_lr = train_one_epoch(model, train_loader, criterion, optimizer, device, scheduler, Plateau = Plateau)\n",
    "    else :\n",
    "      train_loss, avg_lr = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    # Validation Phase\n",
    "    val_loss, prediction = validate(model, val_loader, criterion, device)\n",
    "\n",
    "    # Trackgin loss metrics\n",
    "    tracker.log_epoch(train_loss, val_loss)\n",
    "    if Plateau :\n",
    "      scheduler.step(val_loss)\n",
    "      for param_group in optimizer.param_groups:\n",
    "        avg_lr = param_group['lr']\n",
    "\n",
    "    if scheduler is not None:\n",
    "        tracker.log_lr(avg_lr)\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Avg LR: {avg_lr:.4f}\")\n",
    "    else:\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "    if TRACK_F1 :    \n",
    "      prob_mask = []\n",
    "      for i in range(len(prediction)):\n",
    "          image = torch.sigmoid(prediction[i]).numpy()\n",
    "          prob_mask.append(image)\n",
    "      f1 = th.compute_f1(true_masks=val_mask, pred_probs = prob_mask, patch_size=16, patch_threshold=0.25)\n",
    "      f1_list.append(f1)\n",
    "      print(f\"F1 Score: {f1}\")\n",
    "      if f1 > max_f1:\n",
    "        max_f1 = f1\n",
    "        torch.save(model.state_dict(), f\"saved_models/unet_best_f1.pth\")\n",
    "\n",
    "    # Save the Best Model so far\n",
    "    if val_loss < past_val_loss:\n",
    "        past_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), f\"saved_models/unet_best_val.pth\")\n",
    "        print(\"Model saved\")\n",
    "        if PATCH_SIZE != 400 or PATCH_SIZE != 608:\n",
    "          reconstructed_mask = th.reconstruct_all_masks(\n",
    "              patches=prediction,\n",
    "              num_images= len(val_img),\n",
    "              image_size=(400, 400), \n",
    "              patch_size=PATCH_SIZE,\n",
    "              stride = STRIDE\n",
    "          )\n",
    "        else : \n",
    "          reconstructed_mask = prediction\n",
    "        proba_mask = []\n",
    "        for pred in reconstructed_mask:\n",
    "                pred_tensor = torch.from_numpy(pred).to(device) \n",
    "                proba = torch.sigmoid(pred_tensor)  \n",
    "                proba_mask.append(proba)  \n",
    "\n",
    "        proba_mask_cpu = [proba.cpu().numpy() for proba in proba_mask]\n",
    "        th.plot_images_with_predictions(val_img, val_mask, proba_mask_cpu, rows=2)\n",
    "\n",
    "torch.save(model.state_dict(), \"saved_models/unet_final.pth\")\n",
    "print(\"Training completed - all the models are saved\")\n",
    "\n",
    "tracker.save(th.get_unique_filepath(\"history/training_history.json\"))\n",
    "tracker.plot_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#PRINT F1 SCORE\n",
    "plt.figure(figsize=(10, 6))  \n",
    "plt.plot(list(range(1,NUM_EPOCH+1)), f1_list, marker='o', linestyle='-', label='F1 Score', color = \"orange\")\n",
    "plt.title('F1 Score Over Epochs', fontsize=16)\n",
    "plt.xlabel('Epoch', fontsize=14)\n",
    "plt.ylabel('F1 Score', fontsize=14)\n",
    "plt.ylim(0, 1)\n",
    "plt.yticks(np.linspace(0, 1, 11), fontsize=12) \n",
    "plt.xticks(list(range(0, NUM_EPOCH, max(1, NUM_EPOCH // 10))), fontsize=12)  \n",
    "plt.grid(visible=True, which='both', linestyle='--', linewidth=0.5, alpha=0.7,)\n",
    "plt.legend(fontsize=12)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
